{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19454135-8064-446b-96fa-f056f1ba3b01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T15:39:33.360298Z",
     "iopub.status.busy": "2024-11-18T15:39:33.359879Z",
     "iopub.status.idle": "2024-11-18T15:41:43.813487Z",
     "shell.execute_reply": "2024-11-18T15:41:43.812864Z",
     "shell.execute_reply.started": "2024-11-18T15:39:33.360282Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.5.1-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/notebook/lib/python3.11/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/envs/notebook/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/conda/envs/notebook/lib/python3.11/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/notebook/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/envs/notebook/lib/python3.11/site-packages (from torch) (2024.10.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.1.0 (from torch)\n",
      "  Downloading triton-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/notebook/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading torch-2.5.1-cp311-cp311-manylinux1_x86_64.whl (906.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.5/906.5 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m96.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m86.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m96.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mpmath, triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
      "Successfully installed mpmath-1.3.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 sympy-1.13.1 torch-2.5.1 triton-3.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af575dfd-f7a3-4da9-88d6-7b38e8b148f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T20:54:45.423265Z",
     "iopub.status.busy": "2024-11-18T20:54:45.422834Z",
     "iopub.status.idle": "2024-11-18T20:54:47.132730Z",
     "shell.execute_reply": "2024-11-18T20:54:47.132161Z",
     "shell.execute_reply.started": "2024-11-18T20:54:45.423248Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"This notebook trains and evaluates a convolutional neural network for wavelet analysis.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from copy import deepcopy\n",
    "\n",
    "from model import ConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a9039fb-891e-44db-9160-16cdfcc5d9f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T20:54:47.133763Z",
     "iopub.status.busy": "2024-11-18T20:54:47.133508Z",
     "iopub.status.idle": "2024-11-18T20:54:54.132129Z",
     "shell.execute_reply": "2024-11-18T20:54:54.131447Z",
     "shell.execute_reply.started": "2024-11-18T20:54:47.133743Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cols = [\"ebs\", \"exo\", \"flares\", \"rot\"]\n",
    "cat = pd.DataFrame([], columns=[\"TIC\", \"sector\", *cols, \"wavelet\"])\n",
    "cat.index.name = \"filename\"\n",
    "\n",
    "for i, col in enumerate(cols):\n",
    "    for s in os.listdir(f\"wavelets/tess-{col}\"):\n",
    "        if s not in cat.index:\n",
    "            wav = np.load(f\"wavelets/tess-{col}/{s}\")\n",
    "            cat.loc[s] = [int(s[24:40]), int(s[19:23]), *([0]*len(cols)), wav/wav.max()]\n",
    "        cat.loc[s, col] = 1\n",
    "\n",
    "cat = cat.reset_index().set_index([\"TIC\", \"sector\"]).sort_index()\n",
    "cat[cols] = cat[cols].div(cat[cols].sum(axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b976806b-b46f-4f65-845f-5af9fc019bc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T19:14:44.955968Z",
     "iopub.status.busy": "2024-11-18T19:14:44.955825Z",
     "iopub.status.idle": "2024-11-18T19:14:44.958336Z",
     "shell.execute_reply": "2024-11-18T19:14:44.957788Z",
     "shell.execute_reply.started": "2024-11-18T19:14:44.955954Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# construct list of values \"{tic}_{s}\" for each sector of each TIC ID\n",
    "# unused for now but this might be useful later.\n",
    "# ebs = pd.read_csv(\"catalogs/tess-ebs.csv\")\n",
    "# ebs[\"sectors\"] = ebs[\"sectors\"].apply(lambda x: list(map(int, x.strip(\"[]\").split(\", \"))))\n",
    "# ids = [f\"{row['ID']}_{s}\" for _, row in ebs.iterrows() for s in row['sectors']]\n",
    "# len(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "840decba-f3eb-428d-b23b-b15fd1481652",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T20:54:54.133450Z",
     "iopub.status.busy": "2024-11-18T20:54:54.133116Z",
     "iopub.status.idle": "2024-11-18T20:54:54.137667Z",
     "shell.execute_reply": "2024-11-18T20:54:54.137255Z",
     "shell.execute_reply.started": "2024-11-18T20:54:54.133430Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set global parameters based on command line input\n",
    "RUN_NUMBER = 0 # which channels selection to use\n",
    "BATCH_SIZE = 100\n",
    "PATIENCE = 30\n",
    "MAX_EPOCHS = 500\n",
    "RUN_NAME = \"classes\"\n",
    "\n",
    "OUTPUT_PATH = f\"{RUN_NAME}_{RUN_NUMBER}\"\n",
    "\n",
    "# Channel configurations\n",
    "channels = {\n",
    "    0: [8, 16, 32],\n",
    "    1: [16, 32, 64],\n",
    "    2: [32, 64, 128],\n",
    "    3: [64, 128, 256]\n",
    "}\n",
    "\n",
    "# Create output directories\n",
    "for folder in ['models', 'plots', 'losses', 'output']:\n",
    "    os.makedirs(os.path.join(OUTPUT_PATH, folder), exist_ok=True)\n",
    "\n",
    "selected_channels = channels[RUN_NUMBER]\n",
    "loss_function = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e180490c-b209-4655-818d-c6a2873e5e3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T21:01:14.742635Z",
     "iopub.status.busy": "2024-11-18T21:01:14.742285Z",
     "iopub.status.idle": "2024-11-18T21:01:14.753350Z",
     "shell.execute_reply": "2024-11-18T21:01:14.752854Z",
     "shell.execute_reply.started": "2024-11-18T21:01:14.742617Z"
    }
   },
   "outputs": [],
   "source": [
    "class WaveletDataset(Dataset):\n",
    "    \"\"\"Custom dataset class for loading wavelet data from files.\n",
    "\n",
    "    This class is responsible for loading wavelet data and corresponding labels\n",
    "    from the specified file paths. It supports splitting the data into training,\n",
    "    validation, and test sets.\n",
    "\n",
    "    Attributes:\n",
    "        data_frame (np.ndarray): Array containing the wavelet data.\n",
    "        labels (np.ndarray): Array containing the scaled labels.\n",
    "    \"\"\"    \n",
    "    def __init__(self, data, mode, split=[0.8, 0.1, 0.1]):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "        \n",
    "        Args:\n",
    "            data (DataFrame): the input and output data.\n",
    "            mode (str): Mode to load ('train', 'val', or 'test').\n",
    "            split (list): train, validation, and test split fractions.\n",
    "        \"\"\"\n",
    "        ftrain, fval, ftest = [s/sum(split) for s in split]\n",
    "        i_train = int(ftrain * len(data))\n",
    "        i_val = int((ftrain + fval) * len(data))\n",
    "        \n",
    "        if mode == \"train\":\n",
    "            df = data.iloc[:i_train]\n",
    "        elif mode == \"val\":\n",
    "            df = data.iloc[i_train:i_val]\n",
    "        elif mode == \"test\":\n",
    "            df = data.iloc[i_val:]\n",
    "            \n",
    "        self.features = df[\"wavelet\"].values\n",
    "        self.labels = df[[\"ebs\", \"exo\", \"flares\", \"rot\"]].values\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the length of the dataset.\"\"\"\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Retrieve a single sample and its corresponding label.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the sample data (torch.Tensor) and the \n",
    "                   corresponding label (torch.Tensor).\n",
    "        \"\"\"\n",
    "        X = self.features[idx].astype('float32')\n",
    "        X = torch.unsqueeze(torch.tensor(X), 0)\n",
    "        label = torch.tensor(self.labels[idx].astype('float32'))\n",
    "        return X, label\n",
    "        \n",
    "\n",
    "def train(model, device, train_loader, val_loader, patience, max_epochs, model_name=\"cnn\"):\n",
    "    \"\"\"Train the neural network for the specified number of epochs.\n",
    "\n",
    "    This function orchestrates the training loop, updating model weights based on\n",
    "    the training data, validating the model on a validation set, and handling\n",
    "    early stopping based on validation loss.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The neural network model to be trained.\n",
    "        device (torch.device): The device (CPU or GPU) on which to perform training.\n",
    "        train_loader (DataLoader): DataLoader for the training dataset.\n",
    "        val_loader (DataLoader): DataLoader for the validation dataset.\n",
    "        patience (int): Early stopping patience.\n",
    "        max_epochs (int): Maximum number of training iterations.\n",
    "        model_name (str): The name of the model, used for saving the trained weights.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the best model weights, training losses, and validation losses.\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, factor=0.7, patience=3)\n",
    "    \n",
    "    train_losses, val_losses = [], []\n",
    "    min_loss = float('inf')\n",
    "    early_stopping_count = 0\n",
    "    best_weights = None\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        train_loss = train_epoch(model, device, train_loader, optimizer, epoch)\n",
    "        val_loss = test(model, device, val_loader, epoch, model_name, mode=\"Validation\")\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if val_loss < min_loss:\n",
    "            min_loss = val_loss\n",
    "            early_stopping_count = 0\n",
    "            best_weights = deepcopy(model.state_dict())\n",
    "        else:\n",
    "            early_stopping_count += 1\n",
    "            if early_stopping_count == patience:\n",
    "                print(f\"\\nEarly Stopping. Best Epoch: {epoch - patience} with loss {min_loss:.4f}.\")\n",
    "                break\n",
    "\n",
    "    torch.save(best_weights, f\"{OUTPUT_PATH}/models/{model_name}.pt\")\n",
    "    return best_weights, train_losses, val_losses\n",
    "\n",
    "\n",
    "def train_epoch(model, device, train_loader, optimizer, epoch):\n",
    "    \"\"\"Train the model for one epoch.\n",
    "\n",
    "    This function processes each batch of training data, computes the loss,\n",
    "    and updates the model weights accordingly.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The neural network model to be trained.\n",
    "        device (torch.device): The device (CPU or GPU) for training.\n",
    "        train_loader (DataLoader): DataLoader for the training dataset.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer used for weight updates.\n",
    "        epoch (int): The current epoch number.\n",
    "\n",
    "    Returns:\n",
    "        float: The average loss for the epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    losses = []\n",
    "\n",
    "    ndata = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_function(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        ndata += len(data)\n",
    "        #if (batch_idx * len(data)) % 10000 == 0:\n",
    "        print(\n",
    "            f'Train Epoch: {epoch:3d} [{ndata:6d}/{len(train_loader.dataset)}'\n",
    "            f' ({100*ndata/len(train_loader.dataset):3.0f}%)]\\tLoss: {losses[-1]:.6f}',\n",
    "            end=\"\\r\")\n",
    "\n",
    "    return np.mean(losses)\n",
    "\n",
    "\n",
    "def test(model, device, test_loader, epoch=None, model_name=None, mode=\"Validation\"):\n",
    "    \"\"\"Evaluate the model on the test set.\n",
    "\n",
    "    This function assesses the model's performance on a specified dataset\n",
    "    and computes the average loss. It can also generate a plot of predictions\n",
    "    versus true values.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The neural network model to be evaluated.\n",
    "        device (torch.device): The device (CPU or GPU) for evaluation.\n",
    "        test_loader (DataLoader): DataLoader for the test dataset.\n",
    "        epoch (int, optional): The current epoch number (for labeling purposes).\n",
    "        model_name (str, optional): The name of the model (for labeling purposes).\n",
    "        mode (str, optional): Indicates whether the evaluation is for training, validation, or testing.\n",
    "\n",
    "    Returns:\n",
    "        float: The average loss on the test set.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    targets, preds = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            targets.extend(target.cpu().numpy())\n",
    "            preds.extend(output.cpu().numpy())\n",
    "            test_loss += loss_function(output, target).item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    print(f'\\n                     Average {mode} Loss: {test_loss:.4f}')\n",
    "\n",
    "    if mode.lower() == \"test\":\n",
    "        return np.squeeze(preds), np.squeeze(targets), test_loss\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea0c7ba2-f9fb-42f6-93cb-51bf7a91f19b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T21:01:15.845517Z",
     "iopub.status.busy": "2024-11-18T21:01:15.845059Z",
     "iopub.status.idle": "2024-11-18T21:13:11.516125Z",
     "shell.execute_reply": "2024-11-18T21:13:11.515550Z",
     "shell.execute_reply.started": "2024-11-18T21:01:15.845499Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:   1 [  5559/5559 (100%)]\tLoss: 1.387976\n",
      "                     Average Validation Loss: 1.3874\n",
      "Train Epoch:   2 [  5559/5559 (100%)]\tLoss: 1.386157\n",
      "                     Average Validation Loss: 1.3852\n",
      "Train Epoch:   3 [  5559/5559 (100%)]\tLoss: 1.383518\n",
      "                     Average Validation Loss: 1.3814\n",
      "Train Epoch:   4 [  5559/5559 (100%)]\tLoss: 1.377425\n",
      "                     Average Validation Loss: 1.3756\n",
      "Train Epoch:   5 [  5559/5559 (100%)]\tLoss: 1.377048\n",
      "                     Average Validation Loss: 1.3685\n",
      "Train Epoch:   6 [  5559/5559 (100%)]\tLoss: 1.376382\n",
      "                     Average Validation Loss: 1.3615\n",
      "Train Epoch:   7 [  5559/5559 (100%)]\tLoss: 1.365963\n",
      "                     Average Validation Loss: 1.3546\n",
      "Train Epoch:   8 [  5559/5559 (100%)]\tLoss: 1.365982\n",
      "                     Average Validation Loss: 1.3467\n",
      "Train Epoch:   9 [  5559/5559 (100%)]\tLoss: 1.357942\n",
      "                     Average Validation Loss: 1.3374\n",
      "Train Epoch:  10 [  5559/5559 (100%)]\tLoss: 1.343528\n",
      "                     Average Validation Loss: 1.3249\n",
      "Train Epoch:  11 [  5559/5559 (100%)]\tLoss: 1.305255\n",
      "                     Average Validation Loss: 1.3083\n",
      "Train Epoch:  12 [  5559/5559 (100%)]\tLoss: 1.283756\n",
      "                     Average Validation Loss: 1.2884\n",
      "Train Epoch:  13 [  5559/5559 (100%)]\tLoss: 1.275729\n",
      "                     Average Validation Loss: 1.2682\n",
      "Train Epoch:  14 [  5559/5559 (100%)]\tLoss: 1.258378\n",
      "                     Average Validation Loss: 1.2506\n",
      "Train Epoch:  15 [  5559/5559 (100%)]\tLoss: 1.223405\n",
      "                     Average Validation Loss: 1.2368\n",
      "Train Epoch:  16 [  5559/5559 (100%)]\tLoss: 1.188806\n",
      "                     Average Validation Loss: 1.2260\n",
      "Train Epoch:  17 [  5559/5559 (100%)]\tLoss: 1.194670\n",
      "                     Average Validation Loss: 1.2173\n",
      "Train Epoch:  18 [  5559/5559 (100%)]\tLoss: 1.224249\n",
      "                     Average Validation Loss: 1.2109\n",
      "Train Epoch:  19 [  5559/5559 (100%)]\tLoss: 1.219762\n",
      "                     Average Validation Loss: 1.2052\n",
      "Train Epoch:  20 [  5559/5559 (100%)]\tLoss: 1.194384\n",
      "                     Average Validation Loss: 1.2002\n",
      "Train Epoch:  21 [  5559/5559 (100%)]\tLoss: 1.179005\n",
      "                     Average Validation Loss: 1.1958\n",
      "Train Epoch:  22 [  5559/5559 (100%)]\tLoss: 1.186883\n",
      "                     Average Validation Loss: 1.1921\n",
      "Train Epoch:  23 [  5559/5559 (100%)]\tLoss: 1.177471\n",
      "                     Average Validation Loss: 1.1891\n",
      "Train Epoch:  24 [  5559/5559 (100%)]\tLoss: 1.156718\n",
      "                     Average Validation Loss: 1.1863\n",
      "Train Epoch:  25 [  5559/5559 (100%)]\tLoss: 1.173018\n",
      "                     Average Validation Loss: 1.1835\n",
      "Train Epoch:  26 [  5559/5559 (100%)]\tLoss: 1.171223\n",
      "                     Average Validation Loss: 1.1809\n",
      "Train Epoch:  27 [  5559/5559 (100%)]\tLoss: 1.174740\n",
      "                     Average Validation Loss: 1.1791\n",
      "Train Epoch:  28 [  5559/5559 (100%)]\tLoss: 1.140604\n",
      "                     Average Validation Loss: 1.1767\n",
      "Train Epoch:  29 [  5559/5559 (100%)]\tLoss: 1.178131\n",
      "                     Average Validation Loss: 1.1745\n",
      "Train Epoch:  30 [  5559/5559 (100%)]\tLoss: 1.158283\n",
      "                     Average Validation Loss: 1.1729\n",
      "Train Epoch:  31 [  5559/5559 (100%)]\tLoss: 1.139736\n",
      "                     Average Validation Loss: 1.1706\n",
      "Train Epoch:  32 [  5559/5559 (100%)]\tLoss: 1.170464\n",
      "                     Average Validation Loss: 1.1686\n",
      "Train Epoch:  33 [  5559/5559 (100%)]\tLoss: 1.141063\n",
      "                     Average Validation Loss: 1.1668\n",
      "Train Epoch:  34 [  5559/5559 (100%)]\tLoss: 1.140617\n",
      "                     Average Validation Loss: 1.1655\n",
      "Train Epoch:  35 [  5559/5559 (100%)]\tLoss: 1.165798\n",
      "                     Average Validation Loss: 1.1632\n",
      "Train Epoch:  36 [  5559/5559 (100%)]\tLoss: 1.154664\n",
      "                     Average Validation Loss: 1.1614\n",
      "Train Epoch:  37 [  5559/5559 (100%)]\tLoss: 1.159472\n",
      "                     Average Validation Loss: 1.1601\n",
      "Train Epoch:  38 [  5559/5559 (100%)]\tLoss: 1.128613\n",
      "                     Average Validation Loss: 1.1585\n",
      "Train Epoch:  39 [  5559/5559 (100%)]\tLoss: 1.136662\n",
      "                     Average Validation Loss: 1.1567\n",
      "Train Epoch:  40 [  5559/5559 (100%)]\tLoss: 1.160041\n",
      "                     Average Validation Loss: 1.1558\n",
      "Train Epoch:  41 [  5559/5559 (100%)]\tLoss: 1.145511\n",
      "                     Average Validation Loss: 1.1545\n",
      "Train Epoch:  42 [  5559/5559 (100%)]\tLoss: 1.137967\n",
      "                     Average Validation Loss: 1.1537\n",
      "Train Epoch:  43 [  5559/5559 (100%)]\tLoss: 1.155250\n",
      "                     Average Validation Loss: 1.1512\n",
      "Train Epoch:  44 [  5559/5559 (100%)]\tLoss: 1.134303\n",
      "                     Average Validation Loss: 1.1507\n",
      "Train Epoch:  45 [  5559/5559 (100%)]\tLoss: 1.145062\n",
      "                     Average Validation Loss: 1.1489\n",
      "Train Epoch:  46 [  5559/5559 (100%)]\tLoss: 1.157013\n",
      "                     Average Validation Loss: 1.1483\n",
      "Train Epoch:  47 [  5559/5559 (100%)]\tLoss: 1.128480\n",
      "                     Average Validation Loss: 1.1470\n",
      "Train Epoch:  48 [  5559/5559 (100%)]\tLoss: 1.130517\n",
      "                     Average Validation Loss: 1.1466\n",
      "Train Epoch:  49 [  5559/5559 (100%)]\tLoss: 1.150976\n",
      "                     Average Validation Loss: 1.1461\n",
      "Train Epoch:  50 [  5559/5559 (100%)]\tLoss: 1.140640\n",
      "                     Average Validation Loss: 1.1448\n",
      "Train Epoch:  51 [  5559/5559 (100%)]\tLoss: 1.140686\n",
      "                     Average Validation Loss: 1.1437\n",
      "Train Epoch:  52 [  5559/5559 (100%)]\tLoss: 1.132808\n",
      "                     Average Validation Loss: 1.1425\n",
      "Train Epoch:  53 [  5559/5559 (100%)]\tLoss: 1.151065\n",
      "                     Average Validation Loss: 1.1413\n",
      "Train Epoch:  54 [  5559/5559 (100%)]\tLoss: 1.148500\n",
      "                     Average Validation Loss: 1.1407\n",
      "Train Epoch:  55 [  5559/5559 (100%)]\tLoss: 1.129715\n",
      "                     Average Validation Loss: 1.1397\n",
      "Train Epoch:  56 [  5559/5559 (100%)]\tLoss: 1.155645\n",
      "                     Average Validation Loss: 1.1390\n",
      "Train Epoch:  57 [  5559/5559 (100%)]\tLoss: 1.128781\n",
      "                     Average Validation Loss: 1.1383\n",
      "Train Epoch:  58 [  5559/5559 (100%)]\tLoss: 1.111200\n",
      "                     Average Validation Loss: 1.1368\n",
      "Train Epoch:  59 [  5559/5559 (100%)]\tLoss: 1.170494\n",
      "                     Average Validation Loss: 1.1369\n",
      "Train Epoch:  60 [  5559/5559 (100%)]\tLoss: 1.141062\n",
      "                     Average Validation Loss: 1.1358\n",
      "Train Epoch:  61 [  5559/5559 (100%)]\tLoss: 1.118699\n",
      "                     Average Validation Loss: 1.1347\n",
      "Train Epoch:  62 [  5559/5559 (100%)]\tLoss: 1.120179\n",
      "                     Average Validation Loss: 1.1340\n",
      "Train Epoch:  63 [  5559/5559 (100%)]\tLoss: 1.135334\n",
      "                     Average Validation Loss: 1.1339\n",
      "Train Epoch:  64 [  5559/5559 (100%)]\tLoss: 1.115504\n",
      "                     Average Validation Loss: 1.1334\n",
      "Train Epoch:  65 [  5559/5559 (100%)]\tLoss: 1.131917\n",
      "                     Average Validation Loss: 1.1326\n",
      "Train Epoch:  66 [  5559/5559 (100%)]\tLoss: 1.157850\n",
      "                     Average Validation Loss: 1.1309\n",
      "Train Epoch:  67 [  5559/5559 (100%)]\tLoss: 1.146579\n",
      "                     Average Validation Loss: 1.1324\n",
      "Train Epoch:  68 [  5559/5559 (100%)]\tLoss: 1.137398\n",
      "                     Average Validation Loss: 1.1314\n",
      "Train Epoch:  69 [  5559/5559 (100%)]\tLoss: 1.149513\n",
      "                     Average Validation Loss: 1.1306\n",
      "Train Epoch:  70 [  5559/5559 (100%)]\tLoss: 1.135303\n",
      "                     Average Validation Loss: 1.1286\n",
      "Train Epoch:  71 [  5559/5559 (100%)]\tLoss: 1.138893\n",
      "                     Average Validation Loss: 1.1285\n",
      "Train Epoch:  72 [  5559/5559 (100%)]\tLoss: 1.144949\n",
      "                     Average Validation Loss: 1.1283\n",
      "Train Epoch:  73 [  5559/5559 (100%)]\tLoss: 1.144455\n",
      "                     Average Validation Loss: 1.1279\n",
      "Train Epoch:  74 [  5559/5559 (100%)]\tLoss: 1.123413\n",
      "                     Average Validation Loss: 1.1272\n",
      "Train Epoch:  75 [  5559/5559 (100%)]\tLoss: 1.103361\n",
      "                     Average Validation Loss: 1.1261\n",
      "Train Epoch:  76 [  5559/5559 (100%)]\tLoss: 1.157110\n",
      "                     Average Validation Loss: 1.1264\n",
      "Train Epoch:  77 [  5559/5559 (100%)]\tLoss: 1.137981\n",
      "                     Average Validation Loss: 1.1252\n",
      "Train Epoch:  78 [  5559/5559 (100%)]\tLoss: 1.118222\n",
      "                     Average Validation Loss: 1.1240\n",
      "Train Epoch:  79 [  5559/5559 (100%)]\tLoss: 1.134327\n",
      "                     Average Validation Loss: 1.1236\n",
      "Train Epoch:  80 [  5559/5559 (100%)]\tLoss: 1.122912\n",
      "                     Average Validation Loss: 1.1234\n",
      "Train Epoch:  81 [  5559/5559 (100%)]\tLoss: 1.127114\n",
      "                     Average Validation Loss: 1.1231\n",
      "Train Epoch:  82 [  5559/5559 (100%)]\tLoss: 1.134923\n",
      "                     Average Validation Loss: 1.1221\n",
      "Train Epoch:  83 [  5559/5559 (100%)]\tLoss: 1.136911\n",
      "                     Average Validation Loss: 1.1216\n",
      "Train Epoch:  84 [  5559/5559 (100%)]\tLoss: 1.122003\n",
      "                     Average Validation Loss: 1.1205\n",
      "Train Epoch:  85 [  5559/5559 (100%)]\tLoss: 1.140985\n",
      "                     Average Validation Loss: 1.1208\n",
      "Train Epoch:  86 [  5559/5559 (100%)]\tLoss: 1.138191\n",
      "                     Average Validation Loss: 1.1201\n",
      "Train Epoch:  87 [  5559/5559 (100%)]\tLoss: 1.154512\n",
      "                     Average Validation Loss: 1.1192\n",
      "Train Epoch:  88 [  5559/5559 (100%)]\tLoss: 1.141634\n",
      "                     Average Validation Loss: 1.1186\n",
      "Train Epoch:  89 [  5559/5559 (100%)]\tLoss: 1.153876\n",
      "                     Average Validation Loss: 1.1184\n",
      "Train Epoch:  90 [  5559/5559 (100%)]\tLoss: 1.146595\n",
      "                     Average Validation Loss: 1.1173\n",
      "Train Epoch:  91 [  5559/5559 (100%)]\tLoss: 1.121986\n",
      "                     Average Validation Loss: 1.1167\n",
      "Train Epoch:  92 [  5559/5559 (100%)]\tLoss: 1.132381\n",
      "                     Average Validation Loss: 1.1170\n",
      "Train Epoch:  93 [  5559/5559 (100%)]\tLoss: 1.156885\n",
      "                     Average Validation Loss: 1.1159\n",
      "Train Epoch:  94 [  5559/5559 (100%)]\tLoss: 1.128636\n",
      "                     Average Validation Loss: 1.1159\n",
      "Train Epoch:  95 [  5559/5559 (100%)]\tLoss: 1.113548\n",
      "                     Average Validation Loss: 1.1146\n",
      "Train Epoch:  96 [  5559/5559 (100%)]\tLoss: 1.160002\n",
      "                     Average Validation Loss: 1.1153\n",
      "Train Epoch:  97 [  5559/5559 (100%)]\tLoss: 1.147271\n",
      "                     Average Validation Loss: 1.1148\n",
      "Train Epoch:  98 [  5559/5559 (100%)]\tLoss: 1.164103\n",
      "                     Average Validation Loss: 1.1139\n",
      "Train Epoch:  99 [  5559/5559 (100%)]\tLoss: 1.129327\n",
      "                     Average Validation Loss: 1.1131\n",
      "Train Epoch: 100 [  5559/5559 (100%)]\tLoss: 1.106757\n",
      "                     Average Validation Loss: 1.1129\n",
      "Train Epoch: 101 [  5559/5559 (100%)]\tLoss: 1.143764\n",
      "                     Average Validation Loss: 1.1124\n",
      "Train Epoch: 102 [  5559/5559 (100%)]\tLoss: 1.121689\n",
      "                     Average Validation Loss: 1.1124\n",
      "Train Epoch: 103 [  5559/5559 (100%)]\tLoss: 1.130074\n",
      "                     Average Validation Loss: 1.1119\n",
      "Train Epoch: 104 [  5559/5559 (100%)]\tLoss: 1.151234\n",
      "                     Average Validation Loss: 1.1123\n",
      "Train Epoch: 105 [  5559/5559 (100%)]\tLoss: 1.131055\n",
      "                     Average Validation Loss: 1.1108\n",
      "Train Epoch: 106 [  5559/5559 (100%)]\tLoss: 1.142672\n",
      "                     Average Validation Loss: 1.1110\n",
      "Train Epoch: 107 [  5559/5559 (100%)]\tLoss: 1.141457\n",
      "                     Average Validation Loss: 1.1103\n",
      "Train Epoch: 108 [  5559/5559 (100%)]\tLoss: 1.146915\n",
      "                     Average Validation Loss: 1.1098\n",
      "Train Epoch: 109 [  5559/5559 (100%)]\tLoss: 1.140122\n",
      "                     Average Validation Loss: 1.1108\n",
      "Train Epoch: 110 [  5559/5559 (100%)]\tLoss: 1.142719\n",
      "                     Average Validation Loss: 1.1098\n",
      "Train Epoch: 111 [  5559/5559 (100%)]\tLoss: 1.122126\n",
      "                     Average Validation Loss: 1.1093\n",
      "Train Epoch: 112 [  5559/5559 (100%)]\tLoss: 1.142380\n",
      "                     Average Validation Loss: 1.1089\n",
      "Train Epoch: 113 [  5559/5559 (100%)]\tLoss: 1.146339\n",
      "                     Average Validation Loss: 1.1098\n",
      "Train Epoch: 114 [  5559/5559 (100%)]\tLoss: 1.105235\n",
      "                     Average Validation Loss: 1.1083\n",
      "Train Epoch: 115 [  5559/5559 (100%)]\tLoss: 1.104999\n",
      "                     Average Validation Loss: 1.1090\n",
      "Train Epoch: 116 [  5559/5559 (100%)]\tLoss: 1.123358\n",
      "                     Average Validation Loss: 1.1080\n",
      "Train Epoch: 117 [  5559/5559 (100%)]\tLoss: 1.145397\n",
      "                     Average Validation Loss: 1.1074\n",
      "Train Epoch: 118 [  5559/5559 (100%)]\tLoss: 1.141849\n",
      "                     Average Validation Loss: 1.1073\n",
      "Train Epoch: 119 [  5559/5559 (100%)]\tLoss: 1.132275\n",
      "                     Average Validation Loss: 1.1076\n",
      "Train Epoch: 120 [  5559/5559 (100%)]\tLoss: 1.144261\n",
      "                     Average Validation Loss: 1.1071\n",
      "Train Epoch: 121 [  5559/5559 (100%)]\tLoss: 1.142544\n",
      "                     Average Validation Loss: 1.1070\n",
      "Train Epoch: 122 [  5559/5559 (100%)]\tLoss: 1.114141\n",
      "                     Average Validation Loss: 1.1056\n",
      "Train Epoch: 123 [  5559/5559 (100%)]\tLoss: 1.137310\n",
      "                     Average Validation Loss: 1.1068\n",
      "Train Epoch: 124 [  5559/5559 (100%)]\tLoss: 1.142527\n",
      "                     Average Validation Loss: 1.1059\n",
      "Train Epoch: 125 [  5559/5559 (100%)]\tLoss: 1.154868\n",
      "                     Average Validation Loss: 1.1066\n",
      "Train Epoch: 126 [  5559/5559 (100%)]\tLoss: 1.141590\n",
      "                     Average Validation Loss: 1.1056\n",
      "Train Epoch: 127 [  5559/5559 (100%)]\tLoss: 1.139100\n",
      "                     Average Validation Loss: 1.1068\n",
      "Train Epoch: 128 [  5559/5559 (100%)]\tLoss: 1.157298\n",
      "                     Average Validation Loss: 1.1052\n",
      "Train Epoch: 129 [  5559/5559 (100%)]\tLoss: 1.144380\n",
      "                     Average Validation Loss: 1.1046\n",
      "Train Epoch: 130 [  5559/5559 (100%)]\tLoss: 1.138447\n",
      "                     Average Validation Loss: 1.1049\n",
      "Train Epoch: 131 [  5559/5559 (100%)]\tLoss: 1.131094\n",
      "                     Average Validation Loss: 1.1053\n",
      "Train Epoch: 132 [  5559/5559 (100%)]\tLoss: 1.161559\n",
      "                     Average Validation Loss: 1.1051\n",
      "Train Epoch: 133 [  5559/5559 (100%)]\tLoss: 1.130744\n",
      "                     Average Validation Loss: 1.1047\n",
      "Train Epoch: 134 [  5559/5559 (100%)]\tLoss: 1.114036\n",
      "                     Average Validation Loss: 1.1047\n",
      "Train Epoch: 135 [  5559/5559 (100%)]\tLoss: 1.140883\n",
      "                     Average Validation Loss: 1.1045\n",
      "Train Epoch: 136 [  5559/5559 (100%)]\tLoss: 1.122468\n",
      "                     Average Validation Loss: 1.1046\n",
      "Train Epoch: 137 [  5559/5559 (100%)]\tLoss: 1.124443\n",
      "                     Average Validation Loss: 1.1042\n",
      "Train Epoch: 138 [  5559/5559 (100%)]\tLoss: 1.136469\n",
      "                     Average Validation Loss: 1.1038\n",
      "Train Epoch: 139 [  5559/5559 (100%)]\tLoss: 1.122056\n",
      "                     Average Validation Loss: 1.1037\n",
      "Train Epoch: 140 [  5559/5559 (100%)]\tLoss: 1.140550\n",
      "                     Average Validation Loss: 1.1042\n",
      "Train Epoch: 141 [  5559/5559 (100%)]\tLoss: 1.141289\n",
      "                     Average Validation Loss: 1.1037\n",
      "Train Epoch: 142 [  5559/5559 (100%)]\tLoss: 1.145963\n",
      "                     Average Validation Loss: 1.1034\n",
      "Train Epoch: 143 [  5559/5559 (100%)]\tLoss: 1.150323\n",
      "                     Average Validation Loss: 1.1038\n",
      "Train Epoch: 144 [  5559/5559 (100%)]\tLoss: 1.102408\n",
      "                     Average Validation Loss: 1.1032\n",
      "Train Epoch: 145 [  5559/5559 (100%)]\tLoss: 1.154817\n",
      "                     Average Validation Loss: 1.1028\n",
      "Train Epoch: 146 [  5559/5559 (100%)]\tLoss: 1.129153\n",
      "                     Average Validation Loss: 1.1032\n",
      "Train Epoch: 147 [  5559/5559 (100%)]\tLoss: 1.133874\n",
      "                     Average Validation Loss: 1.1029\n",
      "Train Epoch: 148 [  5559/5559 (100%)]\tLoss: 1.132618\n",
      "                     Average Validation Loss: 1.1029\n",
      "Train Epoch: 149 [  5559/5559 (100%)]\tLoss: 1.127889\n",
      "                     Average Validation Loss: 1.1028\n",
      "Train Epoch: 150 [  5559/5559 (100%)]\tLoss: 1.134928\n",
      "                     Average Validation Loss: 1.1030\n",
      "Train Epoch: 151 [  5559/5559 (100%)]\tLoss: 1.123212\n",
      "                     Average Validation Loss: 1.1032\n",
      "Train Epoch: 152 [  5559/5559 (100%)]\tLoss: 1.123214\n",
      "                     Average Validation Loss: 1.1029\n",
      "Train Epoch: 153 [  5559/5559 (100%)]\tLoss: 1.132100\n",
      "                     Average Validation Loss: 1.1030\n",
      "Train Epoch: 154 [  5559/5559 (100%)]\tLoss: 1.131737\n",
      "                     Average Validation Loss: 1.1028\n",
      "Train Epoch: 155 [  5559/5559 (100%)]\tLoss: 1.149029\n",
      "                     Average Validation Loss: 1.1031\n",
      "Train Epoch: 156 [  5559/5559 (100%)]\tLoss: 1.134616\n",
      "                     Average Validation Loss: 1.1025\n",
      "Train Epoch: 157 [  5559/5559 (100%)]\tLoss: 1.136945\n",
      "                     Average Validation Loss: 1.1026\n",
      "Train Epoch: 158 [  5559/5559 (100%)]\tLoss: 1.138819\n",
      "                     Average Validation Loss: 1.1024\n",
      "Train Epoch: 159 [  5559/5559 (100%)]\tLoss: 1.147543\n",
      "                     Average Validation Loss: 1.1026\n",
      "Train Epoch: 160 [  5559/5559 (100%)]\tLoss: 1.143876\n",
      "                     Average Validation Loss: 1.1024\n",
      "Train Epoch: 161 [  5559/5559 (100%)]\tLoss: 1.153931\n",
      "                     Average Validation Loss: 1.1025\n",
      "Train Epoch: 162 [  5559/5559 (100%)]\tLoss: 1.154142\n",
      "                     Average Validation Loss: 1.1024\n",
      "Train Epoch: 163 [  5559/5559 (100%)]\tLoss: 1.144260\n",
      "                     Average Validation Loss: 1.1024\n",
      "Train Epoch: 164 [  5559/5559 (100%)]\tLoss: 1.123666\n",
      "                     Average Validation Loss: 1.1025\n",
      "Train Epoch: 165 [  5559/5559 (100%)]\tLoss: 1.122249\n",
      "                     Average Validation Loss: 1.1025\n",
      "Train Epoch: 166 [  5559/5559 (100%)]\tLoss: 1.131621\n",
      "                     Average Validation Loss: 1.1026\n",
      "Train Epoch: 167 [  5559/5559 (100%)]\tLoss: 1.123380\n",
      "                     Average Validation Loss: 1.1026\n",
      "Train Epoch: 168 [  5559/5559 (100%)]\tLoss: 1.127953\n",
      "                     Average Validation Loss: 1.1026\n",
      "Train Epoch: 169 [  5559/5559 (100%)]\tLoss: 1.129493\n",
      "                     Average Validation Loss: 1.1024\n",
      "Train Epoch: 170 [  5559/5559 (100%)]\tLoss: 1.133130\n",
      "                     Average Validation Loss: 1.1024\n",
      "Train Epoch: 171 [  5559/5559 (100%)]\tLoss: 1.153140\n",
      "                     Average Validation Loss: 1.1022\n",
      "Train Epoch: 172 [  5559/5559 (100%)]\tLoss: 1.176898\n",
      "                     Average Validation Loss: 1.1022\n",
      "Train Epoch: 173 [  5559/5559 (100%)]\tLoss: 1.115801\n",
      "                     Average Validation Loss: 1.1021\n",
      "Train Epoch: 174 [  5559/5559 (100%)]\tLoss: 1.127638\n",
      "                     Average Validation Loss: 1.1022\n",
      "Train Epoch: 175 [  5559/5559 (100%)]\tLoss: 1.140910\n",
      "                     Average Validation Loss: 1.1022\n",
      "Train Epoch: 176 [  5559/5559 (100%)]\tLoss: 1.141002\n",
      "                     Average Validation Loss: 1.1022\n",
      "Train Epoch: 177 [  5559/5559 (100%)]\tLoss: 1.126825\n",
      "                     Average Validation Loss: 1.1022\n",
      "Train Epoch: 178 [  5559/5559 (100%)]\tLoss: 1.132027\n",
      "                     Average Validation Loss: 1.1022\n",
      "Train Epoch: 179 [  5559/5559 (100%)]\tLoss: 1.146988\n",
      "                     Average Validation Loss: 1.1022\n",
      "Train Epoch: 180 [  5559/5559 (100%)]\tLoss: 1.138547\n",
      "                     Average Validation Loss: 1.1022\n",
      "Train Epoch: 181 [  5559/5559 (100%)]\tLoss: 1.133183\n",
      "                     Average Validation Loss: 1.1021\n",
      "Train Epoch: 182 [  5559/5559 (100%)]\tLoss: 1.128048\n",
      "                     Average Validation Loss: 1.1021\n",
      "Train Epoch: 183 [  5559/5559 (100%)]\tLoss: 1.159038\n",
      "                     Average Validation Loss: 1.1020\n",
      "Train Epoch: 184 [  5559/5559 (100%)]\tLoss: 1.148124\n",
      "                     Average Validation Loss: 1.1020\n",
      "Train Epoch: 185 [  5559/5559 (100%)]\tLoss: 1.131123\n",
      "                     Average Validation Loss: 1.1021\n",
      "Train Epoch: 186 [  5559/5559 (100%)]\tLoss: 1.144286\n",
      "                     Average Validation Loss: 1.1020\n",
      "Train Epoch: 187 [  5559/5559 (100%)]\tLoss: 1.143488\n",
      "                     Average Validation Loss: 1.1021\n",
      "Train Epoch: 188 [  5559/5559 (100%)]\tLoss: 1.154615\n",
      "                     Average Validation Loss: 1.1021\n",
      "Train Epoch: 189 [  5559/5559 (100%)]\tLoss: 1.132902\n",
      "                     Average Validation Loss: 1.1021\n",
      "Train Epoch: 190 [  5559/5559 (100%)]\tLoss: 1.129347\n",
      "                     Average Validation Loss: 1.1021\n",
      "Train Epoch: 191 [  5559/5559 (100%)]\tLoss: 1.096982\n",
      "                     Average Validation Loss: 1.1021\n",
      "Train Epoch: 192 [  5559/5559 (100%)]\tLoss: 1.118986\n",
      "                     Average Validation Loss: 1.1021\n",
      "Train Epoch: 193 [  5559/5559 (100%)]\tLoss: 1.149739\n",
      "                     Average Validation Loss: 1.1022\n",
      "Train Epoch: 194 [  5559/5559 (100%)]\tLoss: 1.135840\n",
      "                     Average Validation Loss: 1.1021\n",
      "Train Epoch: 195 [  5559/5559 (100%)]\tLoss: 1.134527\n",
      "                     Average Validation Loss: 1.1021\n",
      "Train Epoch: 196 [  5559/5559 (100%)]\tLoss: 1.158309\n",
      "                     Average Validation Loss: 1.1021\n",
      "Train Epoch: 197 [  5559/5559 (100%)]\tLoss: 1.137491\n",
      "                     Average Validation Loss: 1.1021\n",
      "Train Epoch: 198 [  5559/5559 (100%)]\tLoss: 1.137649\n",
      "                     Average Validation Loss: 1.1021\n",
      "Train Epoch: 199 [  5559/5559 (100%)]\tLoss: 1.139736\n",
      "                     Average Validation Loss: 1.1021\n",
      "Train Epoch: 200 [  5559/5559 (100%)]\tLoss: 1.144092\n",
      "                     Average Validation Loss: 1.1021\n",
      "Train Epoch: 201 [  5559/5559 (100%)]\tLoss: 1.147626\n",
      "                     Average Validation Loss: 1.1021\n",
      "Train Epoch: 202 [  5559/5559 (100%)]\tLoss: 1.123538\n",
      "                     Average Validation Loss: 1.1021\n",
      "Train Epoch: 203 [  5559/5559 (100%)]\tLoss: 1.128478\n",
      "                     Average Validation Loss: 1.1021\n",
      "Train Epoch: 204 [  5559/5559 (100%)]\tLoss: 1.147216\n",
      "                     Average Validation Loss: 1.1021\n",
      "Train Epoch: 205 [  5559/5559 (100%)]\tLoss: 1.116562\n",
      "                     Average Validation Loss: 1.1021\n",
      "Train Epoch: 206 [  5559/5559 (100%)]\tLoss: 1.134233\n",
      "                     Average Validation Loss: 1.1021\n",
      "Train Epoch: 207 [  5559/5559 (100%)]\tLoss: 1.140432\n",
      "                     Average Validation Loss: 1.1021\n",
      "Train Epoch: 208 [  5559/5559 (100%)]\tLoss: 1.140069\n",
      "                     Average Validation Loss: 1.1021\n",
      "Train Epoch: 209 [  5559/5559 (100%)]\tLoss: 1.110154\n",
      "                     Average Validation Loss: 1.1021\n",
      "Train Epoch: 210 [  5559/5559 (100%)]\tLoss: 1.135263\n",
      "                     Average Validation Loss: 1.1020\n",
      "Train Epoch: 211 [  5559/5559 (100%)]\tLoss: 1.140412\n",
      "                     Average Validation Loss: 1.1020\n",
      "Train Epoch: 212 [  5559/5559 (100%)]\tLoss: 1.125481\n",
      "                     Average Validation Loss: 1.1020\n",
      "Train Epoch: 213 [  5559/5559 (100%)]\tLoss: 1.149363\n",
      "                     Average Validation Loss: 1.1020\n",
      "\n",
      "Early Stopping. Best Epoch: 183 with loss 1.1020.\n",
      "\n",
      "Final Performance!\n",
      "\n",
      "                     Average Training Loss: 1.1072\n",
      "\n",
      "                     Average Validation Loss: 1.1020\n"
     ]
    }
   ],
   "source": [
    "# Create Datasets\n",
    "np.random.seed(42)\n",
    "mycat = cat.sample(frac=1)\n",
    "train_loader = DataLoader(WaveletDataset(mycat, \"train\"), batch_size=BATCH_SIZE)\n",
    "val_loader = DataLoader(WaveletDataset(mycat, \"val\"), batch_size=BATCH_SIZE)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model = ConvNet(channels[RUN_NUMBER], k=3).to(device)\n",
    "\n",
    "# Training\n",
    "weights, train_losses, val_losses = train(model, device, train_loader, val_loader, PATIENCE, MAX_EPOCHS, \"test\")\n",
    "\n",
    "# Evaluate best-fit model\n",
    "model.load_state_dict(weights)\n",
    "print(\"\\nFinal Performance!\")\n",
    "test(model, device, train_loader, mode=\"Training\")\n",
    "test(model, device, val_loader, mode=\"Validation\")\n",
    "\n",
    "# Plot learning curve\n",
    "plt.figure()\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss', alpha=0.5)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"{OUTPUT_PATH}/plots/test_performance.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f32ab3f3-e238-493a-8278-016fdfa1893a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T21:13:30.357877Z",
     "iopub.status.busy": "2024-11-18T21:13:30.357365Z",
     "iopub.status.idle": "2024-11-18T21:13:30.576977Z",
     "shell.execute_reply": "2024-11-18T21:13:30.576514Z",
     "shell.execute_reply.started": "2024-11-18T21:13:30.357859Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction on Test set\n",
      "\n",
      "                     Average Test Loss: 1.1039\n"
     ]
    }
   ],
   "source": [
    "# Predictions on Test set\n",
    "print('\\nPrediction on Test set')\n",
    "test_loader = DataLoader(WaveletDataset(mycat, \"test\"), batch_size=BATCH_SIZE)\n",
    "preds, labels, loss = test(model, device, test_loader, mode=\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5f1e203-cc08-4855-a5f7-65851cc12dd8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T21:13:40.370369Z",
     "iopub.status.busy": "2024-11-18T21:13:40.369934Z",
     "iopub.status.idle": "2024-11-18T21:13:40.392887Z",
     "shell.execute_reply": "2024-11-18T21:13:40.392365Z",
     "shell.execute_reply.started": "2024-11-18T21:13:40.370350Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>ebs_true</th>\n",
       "      <th>exo_true</th>\n",
       "      <th>flares_true</th>\n",
       "      <th>rot_true</th>\n",
       "      <th>ebs_pred</th>\n",
       "      <th>exo_pred</th>\n",
       "      <th>flares_pred</th>\n",
       "      <th>rot_pred</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TIC</th>\n",
       "      <th>sector</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>366972961</th>\n",
       "      <th>25</th>\n",
       "      <td>tess2020133194932-s0025-0000000366972961-0182-...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.999596e-01</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>3.019596e-08</td>\n",
       "      <td>3.104273e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349156098</th>\n",
       "      <th>31</th>\n",
       "      <td>tess2020294194027-s0031-0000000349156098-0198-...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.254193e-04</td>\n",
       "      <td>0.999809</td>\n",
       "      <td>1.567031e-05</td>\n",
       "      <td>4.990502e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139804406</th>\n",
       "      <th>1</th>\n",
       "      <td>tess2018206045859-s0001-0000000139804406-0120-...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.715874e-06</td>\n",
       "      <td>0.999209</td>\n",
       "      <td>3.459247e-04</td>\n",
       "      <td>4.439457e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237913194</th>\n",
       "      <th>28</th>\n",
       "      <td>tess2020212050318-s0028-0000000237913194-0190-...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.455609e-09</td>\n",
       "      <td>0.999967</td>\n",
       "      <td>1.587930e-05</td>\n",
       "      <td>1.732106e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238123653</th>\n",
       "      <th>7</th>\n",
       "      <td>tess2019006130736-s0007-0000000238123653-0131-...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.548079e-04</td>\n",
       "      <td>0.000998</td>\n",
       "      <td>4.702520e-04</td>\n",
       "      <td>9.982773e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264461976</th>\n",
       "      <th>32</th>\n",
       "      <td>tess2020324010417-s0032-0000000264461976-0200-...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.082792e-01</td>\n",
       "      <td>0.018903</td>\n",
       "      <td>4.651374e-03</td>\n",
       "      <td>2.681664e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339960875</th>\n",
       "      <th>7</th>\n",
       "      <td>tess2019006130736-s0007-0000000339960875-0131-...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.784950e-01</td>\n",
       "      <td>0.065435</td>\n",
       "      <td>4.199857e-03</td>\n",
       "      <td>5.187063e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343173162</th>\n",
       "      <th>24</th>\n",
       "      <td>tess2020106103520-s0024-0000000343173162-0180-...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.486069e-05</td>\n",
       "      <td>0.005416</td>\n",
       "      <td>9.091045e-04</td>\n",
       "      <td>9.936206e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350073391</th>\n",
       "      <th>26</th>\n",
       "      <td>tess2020160202036-s0026-0000000350073391-0188-...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.024723e-04</td>\n",
       "      <td>0.000918</td>\n",
       "      <td>2.798000e-04</td>\n",
       "      <td>9.982993e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59843967</th>\n",
       "      <th>6</th>\n",
       "      <td>tess2018349182500-s0006-0000000059843967-0126-...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.018843e-06</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>9.302727e-07</td>\n",
       "      <td>1.737664e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>695 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           filename  ebs_true  \\\n",
       "TIC       sector                                                                \n",
       "366972961 25      tess2020133194932-s0025-0000000366972961-0182-...       1.0   \n",
       "349156098 31      tess2020294194027-s0031-0000000349156098-0198-...       1.0   \n",
       "139804406 1       tess2018206045859-s0001-0000000139804406-0120-...       0.0   \n",
       "237913194 28      tess2020212050318-s0028-0000000237913194-0190-...       0.0   \n",
       "238123653 7       tess2019006130736-s0007-0000000238123653-0131-...       0.0   \n",
       "...                                                             ...       ...   \n",
       "264461976 32      tess2020324010417-s0032-0000000264461976-0200-...       0.0   \n",
       "339960875 7       tess2019006130736-s0007-0000000339960875-0131-...       0.0   \n",
       "343173162 24      tess2020106103520-s0024-0000000343173162-0180-...       0.0   \n",
       "350073391 26      tess2020160202036-s0026-0000000350073391-0188-...       0.0   \n",
       "59843967  6       tess2018349182500-s0006-0000000059843967-0126-...       0.0   \n",
       "\n",
       "                  exo_true  flares_true  rot_true      ebs_pred  exo_pred  \\\n",
       "TIC       sector                                                            \n",
       "366972961 25           0.0          0.0       0.0  9.999596e-01  0.000037   \n",
       "349156098 31           0.0          0.0       0.0  1.254193e-04  0.999809   \n",
       "139804406 1            0.0          1.0       0.0  1.715874e-06  0.999209   \n",
       "237913194 28           1.0          0.0       0.0  5.455609e-09  0.999967   \n",
       "238123653 7            0.0          0.0       1.0  2.548079e-04  0.000998   \n",
       "...                    ...          ...       ...           ...       ...   \n",
       "264461976 32           0.0          0.0       1.0  7.082792e-01  0.018903   \n",
       "339960875 7            0.0          0.0       1.0  8.784950e-01  0.065435   \n",
       "343173162 24           0.0          0.0       1.0  5.486069e-05  0.005416   \n",
       "350073391 26           0.0          0.0       1.0  5.024723e-04  0.000918   \n",
       "59843967  6            1.0          0.0       0.0  1.018843e-06  0.999998   \n",
       "\n",
       "                   flares_pred      rot_pred  \n",
       "TIC       sector                              \n",
       "366972961 25      3.019596e-08  3.104273e-06  \n",
       "349156098 31      1.567031e-05  4.990502e-05  \n",
       "139804406 1       3.459247e-04  4.439457e-04  \n",
       "237913194 28      1.587930e-05  1.732106e-05  \n",
       "238123653 7       4.702520e-04  9.982773e-01  \n",
       "...                        ...           ...  \n",
       "264461976 32      4.651374e-03  2.681664e-01  \n",
       "339960875 7       4.199857e-03  5.187063e-02  \n",
       "343173162 24      9.091045e-04  9.936206e-01  \n",
       "350073391 26      2.798000e-04  9.982993e-01  \n",
       "59843967  6       9.302727e-07  1.737664e-07  \n",
       "\n",
       "[695 rows x 9 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = mycat.iloc[-len(labels):].drop(columns=\"wavelet\")\n",
    "output = output.rename(columns={c: c+\"_true\" for c in cols})\n",
    "output[[c+\"_pred\" for c in cols]] = preds\n",
    "output.to_csv(f\"{OUTPUT_PATH}/output/test_output.csv\")\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
